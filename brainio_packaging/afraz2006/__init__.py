import math
import shutil
from pathlib import Path

import numpy as np
from PIL import Image
from numpy.random.mtrand import RandomState
from sklearn.model_selection import train_test_split
from tqdm import tqdm

from brainio_base.stimuli import StimulusSet

# from Afraz et al., Nature 2006:
# "The images were greyscale photographs of 30 face objects and 60 non-face objects chosen randomly
# from a bank of 600 images."
# "Each signal level was generated by assigning a uniformly distributed greyscale value to X% ofimage pixels,
# where X is the absolute signal level (positive for faces and negative for non-faces).
# Noisy face and non-face images create a continuum of task relevant visual signal extending
# from noiseless faces (100) to full noise images (0) to noiseless non- faces (2100).
# For each signal level, 16 face and 16 non-face images were randomly selected from the image bank."
paper_num_face_objects = 30
paper_num_nonface_objects = 60
ratio = paper_num_face_objects / (paper_num_face_objects + paper_num_nonface_objects)

source_face_directory = Path('/braintree/data2/active/common/labeled_faces_in_the_wild/')
source_imagenet_directory = Path('/braintree/data2/active/common/imagenet_raw/train')
source_imagenet_synsets = [  # from https://gist.github.com/fnielsen/4a5c94eaa6dcdf29b7a62d886f540372
    # object synsets that will hopefully not have faces in them
    'n03376595',  # folding chair
    'n07753275',  # pineapple
    'n02321529',  # sea cucumber
    'n02974003',  # car wheel
    'n03792782',  # mountain bike
]

data_directory = Path(__file__).parent


def train_test_stimuli(test_size=.5, oversample_train_nonnoisy=0.0):
    # from the paper, it is unclear whether monkeys were also trained with a 1:2 face/non-face ratio.
    # we here choose to use the same 1:2 ratio.
    face_paths, nonface_paths = collect_stimuli(num_images=1000, face_nonface_ratio=ratio)
    face_paths, nonface_paths = list(sorted(face_paths)), list(sorted(nonface_paths))
    # convert to StimulusSet
    image_ids = [path.stem for path in face_paths + nonface_paths]
    stimuli = StimulusSet({'image_id': image_ids,
                           'image_label': ['face'] * len(face_paths) + ['nonface'] * len(nonface_paths)})
    stimuli.image_paths = dict(zip(image_ids, face_paths + nonface_paths))
    stimuli.identifier = 'faces_nonfaces'
    # convert to grayscale
    stimuli = make_grayscale(stimuli)
    # split train/test
    train_stimuli, test_stimuli = train_test_split(stimuli, test_size=test_size, random_state=42)
    # add noise
    noisy_train_stimuli = make_noisy('train', stimuli=train_stimuli, oversample_nonnoisy=oversample_train_nonnoisy)
    noisy_test_stimuli = make_noisy('test', stimuli=test_stimuli)
    noisy_train_stimuli.identifier = stimuli.identifier + f'-train{1 - test_size:.2f}_noisy{oversample_train_nonnoisy:.2f}'
    noisy_test_stimuli.identifier = stimuli.identifier + f'-test{test_size:.2f}_noisy'
    return noisy_train_stimuli, noisy_test_stimuli


def collect_stimuli(num_images, face_nonface_ratio):
    # get desired number of paths
    num_faces = int(num_images * face_nonface_ratio)
    num_nonfaces = num_images - num_faces
    faces = sample_faces(num_faces)
    nonfaces = sample_nonfaces(num_nonfaces)
    # copy into working directory
    faces_local = copy_paths(faces, data_directory / 'faces')
    nonfaces_local = copy_paths(nonfaces, data_directory / 'nonfaces')
    return faces_local, nonfaces_local


def make_noisy(identifier, stimuli, oversample_nonnoisy=False, skip_if_exist=True):
    noisy_stimuli = stimuli.copy()
    noisy_stimuli['noise_level'] = np.nan
    target_directory = data_directory / f"noisy-{identifier}-{oversample_nonnoisy}"
    target_directory.mkdir(exist_ok=True)
    # only allow skipping file creation if exact same number of files in target directory
    skip_if_exist = skip_if_exist and len(list(target_directory.iterdir())) == len(stimuli)
    random_state = RandomState(seed=20)
    for image_id in tqdm(stimuli['image_id'].values, desc='make noisy'):
        source_path = stimuli.get_image(image_id)
        target_path = target_directory / source_path.name
        noise_level = random_state.uniform(0, 1) if not oversample_nonnoisy \
            else lopsided_sample(lam=oversample_nonnoisy)
        if not target_path.is_file() or not skip_if_exist:
            # if the file exists, assume that it was created under the exact same conditions and re-use
            image = Image.open(source_path)
            noisy_image = make_noisy_image(np.array(image), noise_level=noise_level, random_state=random_state)
            noisy_image = Image.fromarray(noisy_image)
            noisy_image.save(target_path)
        noisy_stimuli.image_paths[image_id] = target_path
        noisy_stimuli['noise_level'][noisy_stimuli['image_id'] == image_id] = noise_level
    assert not np.isnan(noisy_stimuli['noise_level']).any(), "not all stimuli were processed"
    return noisy_stimuli


def make_noisy_image(image, noise_level, random_state):
    shape = image.shape
    assert len(shape) == 2
    noise_mask = random_state.choice(a=[True, False], size=np.prod(shape), p=[noise_level, 1 - noise_level])
    noise = random_state.uniform(low=0, high=255, size=sum(noise_mask))
    image = image.reshape(-1)
    image[noise_mask] = noise
    image = image.reshape(shape)
    return image


def lopsided_sample(size=None, lam=.5):
    sample = np.random.poisson(lam=lam, size=size)
    clip_max = 5
    sample = np.clip(sample, a_min=None, a_max=clip_max)
    sample = sample / clip_max
    return sample


def sample_faces(num_images):
    # faces are not uniformly distributed
    directories = list(source_face_directory.iterdir())
    num_directories = len(directories)
    images_per_directory = math.ceil(num_images / num_directories)
    paths = []
    random_state = RandomState(1)
    for directory in directories:
        if len(set(paths)) == num_images:
            break
        directory_paths = list((source_face_directory / directory).iterdir())
        paths += [source_face_directory / directory / filename
                  for filename in random_state.choice(directory_paths, images_per_directory, replace=False)]
    paths = list(set(paths))  # get rid of duplicates
    return paths


def sample_nonfaces(num_images):
    # uniform distribution of images
    paths = [source_imagenet_directory / synset / path for synset in source_imagenet_synsets
             for path in (source_imagenet_directory / synset).iterdir()]
    random_state = RandomState(1)
    return random_state.choice(paths, num_images, replace=False)


def make_grayscale(stimulus_set: StimulusSet) -> StimulusSet:
    stimulus_set_grayscale = stimulus_set.copy()
    for image_id in tqdm(stimulus_set_grayscale['image_id'], desc='make grayscale'):
        path = stimulus_set_grayscale.get_image(image_id)
        image = Image.open(path)
        directory_grayscale = Path(str(path.parent) + '-grayscale')
        directory_grayscale.mkdir(exist_ok=True)
        path_grayscale = directory_grayscale / (path.stem + '-grayscale' + path.suffix)
        if not path_grayscale.is_file():  # only convert and save if needed
            image_grayscale = image.convert('L')
            image_grayscale.save(path_grayscale)
        stimulus_set_grayscale.image_paths[image_id] = path_grayscale
    stimulus_set_grayscale.identifier += '-grayscale'
    return stimulus_set_grayscale


def copy_paths(paths, target_directory, skip_if_exist=True):
    if target_directory.is_dir() and len(list(target_directory.iterdir())) == len(paths) and skip_if_exist:
        # files have already been copied
        return list(target_directory.iterdir())
    target_directory.mkdir(exist_ok=False)
    copied_paths = []
    for path in tqdm(paths, desc='copy paths'):
        target_path = target_directory / path.name
        shutil.copy(path, target_path)
        copied_paths.append(target_path)
    return copied_paths


if __name__ == '__main__':
    train_stimulus_set, test_stimulus_set = train_test_stimuli()
